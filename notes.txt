// Suggested literature
-Designing Sound by Andy Farnell
-Machine Musicianship by Robert Rowe,
-The Theory and Techniques of Electronic Music by Miller Puckette: http://msp.ucsd.edu/techniques/latest/book.pdf
-The Computer Music Tutorial by Curtis Roads,
-Handmade Electronic Music: The art of Hardware Hacking by Nicolas Collins


/—————— Day 1
Don Buchla: Electronic instruments 3 different components vs Traditional instruments all-in-one
1. Input structure - physical (can be also virtual)
2. Output structure - Sound generator
3. Connection between the input and output - controller
This separation makes it hard to design an instrument following the preconceptions we have from acoustic instruments.

What is mapping? —> Connection between input and output

Embodied interaction -> Context awareness and bringing a meaning to interaction

/—————— Day 2
van nort, Doug. (2009). Instrumental Listening: sonic gesture as design principle. Organised Sound. 14. 177 - 187. 10.1017/S1355771809000284. 
https://www.researchgate.net/publication/231926625_Instrumental_Listening_sonic_gesture_as_design_principle
Shaeffer introduce that sonic gestures follow the envelope of the overall sound
impulsive > percussive
sustained > continuous
iterative > periodic excitation

However, van Nort defends that the sonic gesture is better understood looking not only at the envelope but also how the morphology of the sound changes in time mass, timbre, motion and grain. 
Later Atltavilla et all suggest that gesture depend also on the sonic affordances and culture. 
“to design instruments requires choices made by the instrument designer while listening to sonic gestures in context.” >> Acoustic musical instruments have evolved because there was a particular use of them in certain context and for example the evolution of the orchestra and concert culture demanded more volume from the previous instruments. 

Alessandro Altavilla, Baptiste Caramiaux, and Atau Tanaka. 2013. Towards Gestural Sonic Affordances. Proceedings of the International Conference on New Interfaces for Musical Expression, Graduate School of Culture Technology, KAIST, pp. 61–64.
https://www.nime.org/proceedings/2013/nime2013_145.pdf
3.2 Mapping Scenarions
Grain can refer to the texture, but as it happens over time it can also refer to the sonic gesture
Conclusion
“participants describe sounds  as  resultant  from  an  action  in  the  case  of impulses  and  iterative  sounds,  while  sustained  sounds  were described  referring  to perceivable modulating  characteristics. This  parallels  to  the  present  case  of  active  gestural  control  of sound”

The idea of granular synthesis is that a sound is sampled at the original speed, but it is played at a different speed from each sample point.

/—————— Day 3
FFT is measurement method
Whole sound spectrum is stored in a buffer. 
This buffer is divided into blocks called bins
The bigger our buffer the more bins, therefore better resolution	
Each bin contains the information of X amount of frequencies and their intensity. 
https://dspguide.com/graphics/F_12_2.gif 
A hanning window is used to smooth non-periodical signals

Uses: sound analysis, sound processing, sound correction

/—————— Day 4
Juan Vasquez on his collage 15
"The tool for audio processing was a phase-vocoder patch in Max7 used to stretch the piece to 5 times the original length without affecting the overall pitch. To achieve this, I designed an algorithm using the external object vb.stretch, designed for Maxby Volker B ̈ohm [49]. This object also allows one to replicate the stretched signal into 6 copies, making it possible to apply an independent pitch shifting value to each of them. In Collage 15, there is a gradual harmonic expansion of the stretched sound material, ultimately building up towards a climax in the closing section."

/—————— Day 5
History of spatial music
https://econtact.ca/7_4/zvonar_spatialmusic.html


Surround Sound mailing list:
https://mail.music.vt.edu/mailman/listinfo/sursound

Ambisonics
Soundfields that can be captured, transformed(rotate, push, mirror, beamformed, etc) and decoded to different loudspeaker setups (binaural included). 
Encoding-Decoding adds flexibility
Accuracy depends on ambisonic order - 1st order feels diffused and the sweet spot is small
It becomes more interesting 3rd order and up
Works the best with strict symmetrical setups.
CPU efficient
Ambisonics with VBAP developed at Aalto: http://research.spa.aalto.fi/projects/sparta_vsts/ 

VBAP, instead of thinking of gain changes of loudspeakers pairs, we thing of virtual sources moving across loudspeaker vectors.
More loudspeakers = better accuracy
Flexible setups
Sweet spot grows with >= 8 speakers
CPU efficient

Azimuth vs panning
One does not kill the other
Panning = gain distribution
Azimuth = horizontal plane angle between listener and sound source


VBAP vs Ambisonics
Small spread can bring coloration and break spatialisation dynamics.
VBAP localises more the source on a particular vector of loudspeakers because of its spread implementation. 
On the contrary, Ambisonics plays in all speakers at the same time. This offer a more spread sound source where the accuracy of location depends on the order and phase transformations. Smoother spatialisation.


DBAP
Measures distance between speakers (not listeners) and virtual sound sources
Asymmetrical setups
Non-dependent on the sweet spot
CPU efficient
Speaker gravity can appear
Sound source representation outside the speaker field might bring gravitation to a particular speaker (Not a problem of the prev. 2 methods because the listeners are not supposed to be outside the speaker array)



